Download Link: https://assignmentchef.com/product/solved-csci567-project-4-part2-k-means
<br>
n this assignment you are asked to implement K-means clustering to identify main clusters inthe data, use the discovered centroid of cluster for classification. Specifically, you will

<ul>

 <li>Implement K-means clustering algorithm to identify clusters in a two-dimensional toy-dataset.</li>

 <li>Implement image compression using K-means clustering algorithm.</li>

 <li>Implement classification using the centroids identified by clustering on digits dataset.</li>

 <li>Implement K-means++ clustering algorithm to identify clusters in a two-dimensional toy-dataset i.e. implement the kmeans++ function to compute the centers.</li>

</ul>

NOTE: You only need to make changes in <a href="http://Kmeans.py">Kmeans.py</a> and use <a href="http://KmeansTest.py">KmeansTest.py</a> for testing purposes and to see your results. You can find all TODO’s sequentially in the <a href="http://Kmeans.py">Kmeans.py</a> file.

Depending on your environment you may need to install the python library named, ”pillow”, which is used by matplotlib to process some of the images needed for this assignment.

You can install it by running ’pip3 install pillow’ in your command line.

<h3 id="dataset-for-k-means-clustering">Dataset for K-Means Clustering</h3>

We will use 2 datasets – 2-D Toy Dataset and Digits datasets for K means part.

Toy Dataset is a two-dimensional dataset generated from 4 Gaussian distributions. We will use thisdataset to visualize the results of our algorithm in two dimensions. You can find it in data_loader.py

We will use digits dataset from sklearn to test K-means based classifier and generate digits usingGaussian Mixture model. Each data point is a 8 × 8 image of a digit. This is similar to MNIST but lesscomplex. There are 10 classes in digits dataset.

Link for Digits dataset: sklearn.datasets.digits <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits</a>

<h2 id="k-means-clustering">1. K Means Clustering</h2>

Recall that for a dataset <span class="katex--inline"><span class="katex"><span class="katex-mathml">x_1, . . . , x_N ∈ R^D</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mpunct">,</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">N</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mrel">∈</span></span><span class="base"><span class="mord"><span class="mord mathdefault">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">D</span></span></span></span></span></span></span></span></span></span></span></span>, the K-means distortion objective is:<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">F({mu_k}, {r_{nk}}) = sum_{i=1}^N sum_{k=1}^K r_{nk} |mu_k- x_n|_2^2 qquad (1)</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mclose">}</span><span class="mpunct">,</span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">k</span></span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mclose">}</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span>1</span></span></span><span class=""><span class="mop op-symbol large-op">∑</span></span><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">N</span></span></span></span><span class="vlist-s">​</span></span></span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">k</span><span class="mrel mtight">=</span>1</span></span></span><span class=""><span class="mop op-symbol large-op">∑</span></span><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">K</span></span></span></span><span class="vlist-s">​</span></span></span></span><span class="mord"><span class="mord mathdefault">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">k</span></span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mord">∥</span><span class="mord"><span class="mord mathdefault">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mbin">−</span></span><span class="base"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mord">∥<span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></span>

where <span class="katex--inline"><span class="katex"><span class="katex-mathml">µ_1, . . . , µ_K</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord">µ<span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mpunct">,</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mord">µ<span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">K</span></span></span></span><span class="vlist-s">​</span></span></span></span></span></span></span></span></span> are centroids of the K clusters and <span class="katex--inline"><span class="katex"><span class="katex-mathml">r_{ik} ∈ {0, 1}</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord"><span class="mord mathdefault">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">k</span></span></span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mrel">∈</span></span><span class="base"><span class="mord">0<span class="mpunct">,</span>1</span></span></span></span></span> represents whether example i belongs to cluster k.

<img decoding="async" data-src="Algo1.png" class="lazyload" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">

 <noscript>

  <img decoding="async" src="Algo1.png">

 </noscript>

<h3 id="implementing-k-means-algorithm">1.1 Implementing k-means++ algorithm</h3>

Recall from lecture Kmeans++. Please refer to the algorithm below. In simple terms, cluster centers are initially chosen at random from the set of input observation vectors, where the probability of choosing vector x is high if x is not near any previously chosen centers.

Here is a one-dimensional example. Our observations are <span class="katex--inline"><span class="katex"><span class="katex-mathml">[0, 1, 2, 3, 4]</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mord">1</span><span class="mpunct">,</span><span class="mord">2</span><span class="mpunct">,</span><span class="mord">3</span><span class="mpunct">,</span><span class="mord">4</span><span class="mclose">]</span></span></span></span></span>. Let the first center, <span class="katex--inline"><span class="katex"><span class="katex-mathml">c1</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">c</span><span class="mord">1</span></span></span></span></span>, be 0. The probability that the next cluster center, <span class="katex--inline"><span class="katex"><span class="katex-mathml">c2</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">c</span><span class="mord">2</span></span></span></span></span>, is x is proportional to <span class="katex--inline"><span class="katex"><span class="katex-mathml">||c1-x||^2</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">c</span><span class="mord">1</span><span class="mbin">−</span></span><span class="base"><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord">∣<span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>. So, <span class="katex--inline"><span class="katex"><span class="katex-mathml">P(c2 = 1) = 1a, P(c2 = 2) = 4a, P(c2 = 3) = 9a, P(c2 = 4) = 16a</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">2</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">2</span><span class="mrel">=</span></span><span class="base"><span class="mord">2</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">4</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">2</span><span class="mrel">=</span></span><span class="base"><span class="mord">3</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">9</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">2</span><span class="mrel">=</span></span><span class="base"><span class="mord">4</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mord">6</span><span class="mord mathdefault">a</span></span></span></span></span>, where <span class="katex--inline"><span class="katex"><span class="katex-mathml">a = 1/(1+4+9+16)</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">a</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mord">/</span><span class="mopen">(</span><span class="mord">1</span><span class="mbin">+</span></span><span class="base"><span class="mord">4</span><span class="mbin">+</span></span><span class="base"><span class="mord">9</span><span class="mbin">+</span></span><span class="base"><span class="mord">1</span><span class="mord">6</span><span class="mclose">)</span></span></span></span></span>.

Suppose <span class="katex--inline"><span class="katex"><span class="katex-mathml">c2 = 4</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">c</span><span class="mord">2</span><span class="mrel">=</span></span><span class="base"><span class="mord">4</span></span></span></span></span>. Then, <span class="katex--inline"><span class="katex"><span class="katex-mathml">P(c3 = 1) = 1a, P(c3 = 2) = 4a, P(c3 = 3) = 1a</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">3</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">3</span><span class="mrel">=</span></span><span class="base"><span class="mord">2</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">4</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord">3</span><span class="mrel">=</span></span><span class="base"><span class="mord">3</span><span class="mclose">)</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mord mathdefault">a</span></span></span></span></span>, where <span class="katex--inline"><span class="katex"><span class="katex-mathml">a = 1/(1+4+1)</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathdefault">a</span><span class="mrel">=</span></span><span class="base"><span class="mord">1</span><span class="mord">/</span><span class="mopen">(</span><span class="mord">1</span><span class="mbin">+</span></span><span class="base"><span class="mord">4</span><span class="mbin">+</span></span><span class="base"><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>.

For more insights, follow this: <a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>

<img decoding="async" data-src="kmeans++.png" class="lazyload" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">

 <noscript>

  <img decoding="async" src="kmeans++.png">

 </noscript>

Implement Algorithm by filling out the TODO parts in function <strong>get_k_means_plus_plus_center_indices</strong> of file <strong><a href="http://kmeans.py">kmeans.py</a></strong>. You can test this function on Vocareum separately.

<pre><code>def get_k_means_plus_plus_center_indices(n, n_cluster, x, generator=np.random):    :param n: number of samples in the data    :param n_cluster: the number of cluster centers required    :param x: data - numpy array of points    :param generator: random number generator from 0 to n for choosing the first cluster at random            The default is np.random here but in grading, to calculate deterministic results,            We will be using our own random number generator.    :return: the center points array of length n_clusters with each entry being index to a sample             which is chosen as centroid.           </code></pre>

If the generator is still not clear, its basically a np.random but helps us control the result during testing. SO wherever you would use np.random, use generator instead.

<h3 id="implementing-k-means-clustering-algorithm">1.2 Implementing K-means clustering algorithm</h3>

Implement Algorithm 1 by filling out the TODO parts (<strong>fit</strong> function) in class <strong>KMeans</strong> of file <strong><a href="http://kmeans.py">kmeans.py</a></strong>. Note the following:

<ul>

 <li>Initialize means by picking self.n_cluster from N data points</li>

 <li>Update means and membership until convergence or until you have made self.max_iter updates.</li>

 <li>return (means, membership, number_of_updates)</li>

 <li>If at some iteration, there exists a cluster k with no points assigned to it, then do not update the centroid of this cluster for this round.</li>

 <li>While assigning a sample to a cluster, if there’s a tie (i.e. the sample is equidistant from two centroids), you should choose the one with smaller index (like what numpy.argmin does).</li>

 <li>For each k, we are trying to compare based on the Euclidean distance.</li>

</ul>

<pre class=" language-undefined"><code class="prism language- language-undefined" style="font-family: 'Roboto Mono', 'Lucida Sans Typewriter', 'Lucida Console', monaco, Courrier, monospace; font-size: inherit; background-color: rgba(0, 0, 0, 0.05); border-radius: 3px; padding: 0.5em; display: block; text-size-adjust: none; overflow-x: auto; white-space: pre;">Class KMeans:       Attr:           n_cluster - Number of cluster for kmeans clustering (Int)           max_iter - maximum updates for kmeans clustering (Int)           e - error tolerance (Float)           generator - random number generator from 0 to n for choosing the first cluster at random               The default is np.random here but in grading, to calculate deterministic results,               We will be using our own random number generator.                      def __init__(self, n_cluster, max_iter=100, e=0.0001, generator=np.random):               self.n_cluster = n_cluster               self.max_iter = max_iter               self.e = e               self.generator = generator                        def fit(self, x, centroid_func=get_lloyd_k_means):               Finds n_cluster in the data x               params:                x - N X D numpy array               centroid_func - To specify which algorithm we are using to compute the centers(Lloyd(regular) or Kmeans++) The default is Lloyd's Kmeans.                              returns: A tuple (centroids a n_cluster X D numpy array, y a length (N,) numpy array where cell i is the ith sample's assigned cluster, number_of_updates a Int)           Note: Number of iterations is the number of time you update the assignment</code></pre>

After you complete the implementation, run <a href="http://KmeansTest.py">KmeansTest.py</a> to see the results of this on toydataset. You should be able to see three images generated in plots folder. In particular, you can seetoy dataset predicted labels.png and toy dataset real labels.png and compare the clusters identified by the algorithm against the real clusters. Your implementation should be able to recover the correct clusters sufficiently well. Representative images are shown in fig. 2. Red dots are cluster centroids.Note that color coding of recovered clusters may not match that of correct clusters. This is due to mis-matchin ordering of retrieved clusters and correct clusters (which is fine).

<img decoding="async" data-src="PA4img.png" class="lazyload" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">

 <noscript>

  <img decoding="async" src="PA4img.png">

 </noscript>

<h3 id="classification-with-k-means">1.3 Classification with k-means</h3>

Another application of clustering is to obtain a faster version of the nearest neighbor algorithm. Recall that nearest neighbor evaluates the distance of a test sample from every training point to predict its class, which can be very slow. Instead, we can compress the entire training dataset to just the K centroids, where each centroid is now labeled as the majority class of the corresponding cluster. After this compression the prediction time of nearest neighbor is reduced from O(N) to just O(K) (see Algorithm 2 for the pseudocode).

<img decoding="async" data-src="Algo2.png" class="lazyload" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">

 <noscript>

  <img decoding="async" src="Algo2.png">

 </noscript>

Complete the <strong>fit</strong> and <strong>predict</strong> function in <strong>KMeansClassifier</strong> in file <strong><a href="http://kmeans.py">kmeans.py</a></strong> . Once completed,run <strong><a href="http://KmeansTest.py">KmeansTest.py</a></strong> to evaluate the classifier on a test set (digits). For comparison, the script will also print accuracy of a logistic classifier and a nearest neighbor classifier. (Note: a naive K-means classifier may not do well but it can be an effective unsupervised method in a classification pipeline .)

Note: 1) break ties in the same way as in previous problems; 2) if some centroid doesn’t contain anypoint, set the label of this centroid as 0.

The prediction accuracy baseline is 0.77 for KMeans Lloyd(regular) algorithm and 0.72 for KMeans++ algorithm. Note: these differ on different datasets and in more cases Kmeans++ works better.

<pre><code>    Class KMeansClassifier:                Attr:            n_cluster - Number of cluster for kmeans clustering (Int)            max_iter - maximum updates for kmeans clustering (Int)            e - error tolerance (Float)            generator - random number generator from 0 to n for choosing the first cluster at random            The default is np.random here but in grading, to calculate deterministic results,            We will be using our own random number generator.            def __init__(self, n_cluster, max_iter=100, e=1e-6, generator=np.random):            self.n_cluster = n_cluster            self.max_iter = max_iter            self.e = e            self.generator = generator        def fit(self, x, y, centroid_func=get_lloyd_k_means):                    Train the classifier            params:                x - N X D size  numpy array                y - (N,) size numpy array of labels                centroid_func - To specify which algorithm we are using to compute the centers(Lloyd(regular) or Kmeans++) The default is Lloyd's Kmeans.            returns:                None            Stores following attributes:                self.centroids : centroids obtained by kmeans clustering (n_cluster X D numpy array)                self.centroid_labels : labels of each centroid obtained by                     majority voting (N,) numpy array)                                  def predict(self, x):                    Predict function            params:                x - N X D size  numpy array            returns:                predicted labels - numpy array of size (N,)                       </code></pre>

<h3 id="image-compression-with-k-means">1.4 Image compression with K-means</h3>

In this part, we will look at lossy image compression as an application of clustering. The idea is simply to treat each pixel of an image as a point <span class="katex--inline"><span class="katex"><span class="katex-mathml">x_i</span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class=""><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span></span></span></span></span></span></span></span>, then perform K-means algorithm to cluster these points, and finally replace each pixel with its centroid.

What you need to implement is to compress an image with K centroids given. Specifically, complete thefunction <strong>transform_image</strong> in the file <strong><a href="http://kmeans.py">kmeans.py</a></strong>. You have to reduce the image pixels and size by replacing each RGB values with nearest code vectors based on Euclidean distance.

After your implementation, and after completing Kmeans class, when you run <a href="http://KmeansTest.py">KmeansTest.py</a>, you should be able to see an image compressed_baboon.png in the plots folder. You can see that this image is distorted as compared to the original baboon.tiff.

The ideal result should take about 35-40 iterations and the Mean Square Error should be less than 0.0098. It takes about 1-2 minutes to complete normally.

<pre><code>def transform_image(image, code_vectors):        Quantize image using the code_vectors        Return new image from the image by replacing each RGB value in image with nearest code vectors (nearest in euclidean distance sense)        returns:            numpy array of shape image.shape</code></pre>